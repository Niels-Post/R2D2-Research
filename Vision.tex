%! Author = R2D2 Team 3
%! Date = 14/04/2020

% Preamble
\documentclass[11pt]{article}
\usepackage{hyperref}
\usepackage[official]{eurosym}

\usepackage{graphicx}
\usepackage{tabularx}
\graphicspath{ {./Images/} }

\input{Include/main.tex}


\title{Een methode voor pijndetectie door middel van Computer Vision, ge\"{o}rienteerd op embedded systemen}

\author{\emph{R2D2 Team 3} \and Otto de Visser \and Niels Post}

% Document
\begin{document}

    \begin{titlepage}
        \centering
        \maketitle
        \includegraphics[height=0.6\textheight]{Images/vision.jpg}
        \clearpage
    \end{titlepage}


    \clearpage
    \tableofcontents

    \clearpage


    \section{Abstract}\label{sec:abstract}
    \emph{Work in Progress}
    In dit paper vergelijken wij twee methoden van pijndetectie die werken met Vision.
    Door een literatuuronderzoek stellen wij een basisbegrip van beiden methoden op, en vervolgens vergelijken wij implementaties van de methoden op basis van een aantal vereisten.
    Ook vergelijken wij een set camera's voor algemeen gebruik bij Vision.
    De methode X lijkt beter te zijn omdat Y.
    Camera Z komt als beste uit de test.


    \section{Voorwoord}\label{sec:voorwoord}
    Dit onderzoek wordt uitgevoerd in opdracht van de Hogeschool Utrecht, voor het project R2D2 2020.
    Bij dit project wordt er een ICT-bedrijf gesimuleerd.
    De werkgroep waaronder dit onderzoek valt is Team 3:\\
    \\

    \emph{
        \begin{tabularx}{\textwidth}{YYY}
            Niels Post & Otto de Visser & Amrit Malhi\\
            Menno van der Jagt & Youri de Vor & Finn Fonteijn\\
            Vincent van Setten & Oscar Kromhout
        \end{tabularx}
        \vspace{1em}
    }


    \section{Inleiding}\label{sec:inleiding}

    \emph{R2D2 heeft als bedrijf het doel om multifunctionele, modulaire robots te maken voor gebruik in een rampgebied.
    Omdat in een rampgebied vaak slachtoffers zijn die door allerlei oorzaken hevige pijn hebben,
    is het belangrijk dit te indexeren, en hiernaar prioriteiten te stellen.}

    In dit onderzoek willen wij manieren vinden om door middel van Computer Vision in een klein (embedded) systeem pijnniveaus
    te detecteren.
    Vanwege mogelijke beperkte connectiviteit in een rampgebied onderzoeken wij methoden die volledig automatisch,
    zonder netwerkconnectiviteit of zware hardware kunnen werken.

    Tijdens het lezen van dit onderzoek zal de lezer in de eerste plaats meer leren over vision-algoritmes met pijn
    herkennen als doel.
    Hierbij gaan wij specifiek in op de effici\"{e}ntie, geschiktheid voor embedded systemen, en verwerkingstijden van hardware.
    Hiernaast bespreken wij welke camera's het meest geschikt zijn voor gebruik bij Computer Vision in het algemeen.
    Uiteindelijk kiezen wij 1 combinatie van een algoritme en camera die het best passen bij de context van een pijn
    detectie module binnen een R2D2 robot.


    Om dit onderzoek te defini\"{e}ren zullen wij uit gaan van de volgende onderzoekshoofdvraag:\\

    \begin{definition}
        Hoofdvraag & Welke methoden op basis van computer vision bestaan er zonder gebruik van netwerkconnectiviteit en high performance hardware waarmee je pijn kan kwantificeren in de context van een embedded systeem?
    \end{definition}


    \bigskip

    Deze hoofdvraag verdelen wij in de volgende 7 deelvragen:

    \begin{enumerate}
        \item\label{itm:dv1} Is het mogelijk om computer vision toe te passen op videobeeld om pijn te kunnen herkennen op het gezicht van een mens?
        \item\label{itm:dv2} Hoeveel foto's per tijdseenheid moeten we minimaal verwerken om zeker te zijn dat we een valide uitspraak kunnen doen over het pijnniveau dat iemand ervaart?
        \item\label{itm:dv3} Wat zijn de minimale hardwarevereisten van een systeem dat pijnmetingen door middel van Computer Vision uitvoert?
        \item\label{itm:dv4} Volgt er een toename in accuraatheid uit het langer analyseren van een frame?
        \item\label{itm:dv5} Wat is de meest geschikte camera voor het toepassen van Computer Vision facial tracking algoritmes met embedded systemen die verkrijgbaar is op de markt?
        \item\label{itm:dv6} Welke bestaande implementaties van een pijnmeting algoritme zijn er, en wat zijn de gebruiksrechten hiervan?
        \item\label{itm:dv7} Wat is de maximaal bereikbare accuraatheid en snelheid bij gebruik van boven onderzochte methoden?
    \end{enumerate}


    \section{Probleemstelling}\label{sec:probleemstelling}
    Voor doktoren is het lastig om een objectief pijn vast te stellen.
    Vaak moet een pati\"{e}nt zelfeen getal tussen de 1 en de 10 geven voor zijn pijn.
    Echter is deze manier van pijn inschatten niet erg objectief, daarnaast is het niet
    altijd mogelijk met de pati\"{e}nt te communiceren.
    In rampscenario's komt het vaak voor dat slachtoffers moeilijk bereikbaar zijn, of dat er niet genoeg dokters zijn om elk slachtoffer apart een dokter toe te sturen.
    Wel is het belangrijk om te weten hoeveel pijn mensen hebben om ze met meer/minder prioriteit te helpen.
    Hierom bestaat de vraag naar een automatische module die pijn detecteert.


    \section{Theorie en hypothese}\label{sec:theorie-en-hypothese}
    De hoofdvraag van dit onderzoek vergelijkt methoden die wij nog niet kennen voor het onderzoek.
    Hierom kunnen wij geen hypothese stellen voor de hoofdvraag.
    Wel kunnen wij een set kleinere hypotheses stellen, betreffende enkele deelvragen.

    \paragraph{\hyperref[itm:dv1]{Deelvraag 1}}
    Van wat wij zelf weten van pijndetectie en herkenning, is dat dit een moeilijk onderwerp is.
    Ondanks dat er universele uitingen van pijn zijn (zoals auw zeggen, en een vertrokken gezicht), is het moeilijk om vast te stellen hoe veel pijn iemand heeft.
    Doktoren vragen hierom dan ook vaak aan mensen om hun pijn een cijfer te geven.\\


    \begin{definition}
        Hypothese & Pijn detecteren met Vision is in algemene zin mogelijk, maar het niveau classificeren is waarschijnlijk lastig of onmogelijk
    \end{definition}


    \paragraph{\hyperref[itm:dv3]{Deelvraag 3}}
    Wij verwachten dat de belangrijkste vereiste van een systeem een minimale hoeveelheid werkgeheugen is.
    Omdat er geen directe deadline is voor een berekening, kan een processor hier langer over doen, maar als het geheugen niet toereikend is, kan een afbeelding niet opgeslagen worden voor verwerking.

    Microcontrollers hebben vaak weinig (minder dan 100kB) werkgeheugen.
    Het bijplaatsen van geheugen is maar beperkt mogelijk.
    Een limiterende factor heirvoor is namelijk dat microcontrollers vaak een 16 bits processor hebben, en deze niet meer dan +- 131 kB aan geheugen kunnen adresseren.
    Hierom verwachten wij dat de meeste microcontrollers geen effectieve Vision algoritmes kunnen draaien.
    Hiermee bedoelen we dat het uitvoeren van Vision algoritmen wel mogelijk is, maar dat deze niet op een zodanige kwaliteit (resolutie) uitgevoerd kunnen worden dat het resultaat accuraat is.


    \begin{definition}
        Hypothese & Voor een Vision-gebaseerd pijnmetingsalgoritme is een embedded systeem nodig wat minimaal een 32 bits processor heeft.
    \end{definition}

    \paragraph{\hyperref[itm:dv4]{Deelvraag 4}}
    Wij zijn tot nu toe via lessen bij het vak Computer Vision in aanraking geweest met een aantal Computer Vision algoritmes.
    Een deel hiervan hebben wij zelf uitgewerkt in de huiswerkopdracht gezichtsdetectie, en een deel hebben wij alleen in theorie kort behandeld.
    Bij deze algoritmes , was de uitkomst steeds gebaseerd op een vaste set handelingen.
    Omdat deze handelingen niet afhankelijk zijn van entropie, was het resultaat altijd hetzelfde.
    Wij denken dat de handelingen voor pijndetectie op dezelfde manier zullen werken, aangezien deze ook te maken zullen hebben met het detecteren van gezichtskenmerken.\\

    \begin{definition}
        Hypothese & Het langer analyseren van een videoframe zal geen accuratere resultaten geven bij een pijnmetingsalgoritme op basis van computer vision
    \end{definition}

    \paragraph{\hyperref[itm:dv5]{Deelvraag 5}}
    Computer Vision algoritmes zijn hevig afhankelijk van detectie van kleuren, contrasten en randen.
    Omdat de verwerking hiervan afhankelijk zijn van het kleurpalet van de gebruikte camera, denken wij dat er camera's zullen zijn die specifiek gericht zijn op Computer Vision.
    Mogelijk gebruiken deze camera's geen RGB, maar een uitgebreider kleurenschema, zoals bijvoorbeeld ARGB.\\

    \begin{definition}
        Hypothese & Er zijn camera's op de markt beschikbaar die specifiek gericht zijn op Computer Vision algoritmes.
    \end{definition}


    \section{Begrippenlijst}\label{sec:begrippenlijst}

    \begin{definition}
        Computer Vision & Een algemene term voor algoritmes die gebruik maken van camerabeelden, en hier conclusies uit trekken\\

    \end{definition}


    \section{Uitvoering}\label{sec:uitvoering}

    \subsection{Deelvraag 1 tot en met deelvraag 4}\label{subsec:deelvraag-1-tot-en-met-deelvraag-4}
    Voor deze deelvragen zullen wij een literatuuronderzoek
    verrichten naar methoden van pijndetectie door middel van Vision.
    Hierbij zullen wij per methode de deelvragen beantwoorden, om zo uiteindelijk een conclusie te kunnen trekken over
    de meest geschikte methode(n).
    Voor het aangeven van de hardware-vereisten maken wij een beredenering waarom wij denken dat er een bepaalde hardware-vereiste is.
    In deze beredenering kijken we vooral naar het geheugengebruik van de methode,
    aangezien dit de meest limiterende factor is van een embedded systeem.

    Om de onderzochte methoden inzichtelijk te maken voor onszelf, beantwoorden wij voor onszelf een aantal vragen over elke methode.
    Omdat de antwoorden op deze vragen niet direct relateren aan de deelvragen, maar een methode inzichtelijk dienen te maken, kan de lezer de antwoorden hierop vinden inde bijlagen.
    Deze vragen zijn als volgt:
    \begin{itemize}
        \item Korte Omschrijving
        \item Van welk principe maakt het gebruik
        \item Geschatte piek-geheugengebruik per meting
        \item Geschatte tijdsduur voor een conclusie
        \item Mogelijkheid om te draaien op een embedded systeem
    \end{itemize}


    \subsection{Deelvraag 5}\label{subsec:uitvoering-deelvraag-5}

    Voor \emph{\hyperref[itm:dv1]{deelvraag 5}} doen wij een vergelijkend onderzoek voor camera's.
    We onderzoeken het bestaan van camera's specifiek voor vision, en noteren specificaties van deze camera's.
    Bij dit onderzoek betrekken wij ook een webcam en een actiecamera.
    In dit onderzoek bepalen wij een aantal attributen die de camera moet/mag hebben voor werking bij computer vision op een embedded systeem.
    Volgens deze attributen maken wij een vergelijking van de camera's.

    \subsection{Deelvraag 7}\label{subsec:uitvoering-deelvraag-6}

    Voor deelvraag 7 doen wij experimenteel onderzoek.
    Door meerdere implementaties te vergelijken, kunnen wij de maximale bereikbare waarden ervan vergelijken.


    \section{Resultaten}\label{sec:resultaten}

    \subsection{Deelvraag 1 t/m 4}\label{subsec:deelvraag-1-t/m-4}
    \emph{Is het mogelijk om computer vision toe te passen op videobeeld om pijn te kunnen herkennen op het gezicht van een mens?}

    Wij hebben een aantal velden opgesteld die wij per methode hebben onderzocht.
    De volgende resultaten zijn hieruit gekomen

    \subsubsection{Methode 1: Facial Feature detection}
    \emph{\citet{werner2014automatic}} Automatic pain recognition from video and biomedical signals\\
    \emph{\citet{prkachin1992consistency}} The consistency of facial expressions of pain: a comparison across modalities\\
    Deze methode combineert het gebruik van Vision algoritmen, met het meten van biomedische data door middel van sensoren.
    Omdat dit onderzoek Vision beschrijft, zullen wij het sensoraspect van dit onderzoek niet verwerken. In plaats daarvan gaan we dieper in op de referenties gemaakt in het paper met betrekking tot Vision.

    \paragraph{Aantal foto's per tijdseenheid}
    Deze methode maakt gebruik van veranderingen in gezichtsafstanden ten opzichte van een baseline, en kan dus met 2 foto's een resultaat geven.
    Deze foto's hoeven dan ook niet extreem snel verwerkt te worden, en het systeem mag hier een aantal seconden over doen.

    \paragraph{Hardware-vereisten}\label{meth1-hardware}
    De vereisten voor de berekening zelf zijn weinig: Er moeten slechts een aantal afstanden van coordinaten vergeleken worden.
    Dit zullen de meeste microcontrollers ook kunnen.
    Het probleem zit hierom meer in de methode van facial feature detection.
    Afhankelijk van de gebruikte methode, moet de processor mogelijk meerdere kleurafbeeldingen opslaan, en bij convolutie van een afbeelding moet een afbeelding vaak twee keer aanwezig zijn in het geheugen.

    \emph{Bij de onderstaande redenering gaan wij uit van onze ervaring in algemene gezichtsdetectie, aangezien wij geen diepgaande ervaring hebben met gezichtseigenschappen-detectie.
    Mogelijk is zo een algoritme dus meer/minder geheugenintensief dan wij aannemen}

    Bij het opslaan van een kleine kleurafbeelding (50x50) is al gauw 7.5KB per afbeelding nodig. (2500 pixels van 3 bytes (rgb))
    Omdat naast een aantal van deze afbeeldingen en eventuele afbeeldingsbuffers ook nog de programmacode aanwezig moet zijn op de microcontroller,
    verwachten wij dat dat deze methode niet zal kunnen draaien op een standaard microcontroller.


    Mogelijk zijn uitgebreidere microcontrollers wel toereikend.
    Een Arduino Due heeft bijvoorbeeld 96kB werkgeheugen.
    De marge vinden wij hier echter te klein van.
    Bij het toevoegen van meer functionaliteit zou ook het geheugen van een Due snel volraken.
    Om deze reden raden wij aan om deze methode te draaien op een embedded systeem met werkgeheugen op het circuitboard.
    Denk hierbij aan een Raspberry Pi of een Nvidia Jetson-systeem

    \paragraph{Langer analyseren frame}
    Of pijn accurater gedetecteerd kan worden is grotendeels afhankelijk van de gebruikte methode voor facial feature recognition.
    Het langer analyseren kan wel effect hebben op de afstand waarop pijn gedetecteerd kan worden.
    Dit zou mogelijk zijn door de afbeelding op volledige resolutie te verwerken, en niet omlaag te schalen.

    \subsubsection{Methode 2: Optical Flow}
    \emph{\citet{werner2014automatic}} Automatic pain recognition from video and biomedical signals\\
    \emph{\citet{prkachin1992consistency}} The consistency of facial expressions of pain: a comparison across modalities\\
    Deze methode combineert het gebruik van Vision algoritmen, met het meten van biomedische data door middel van sensoren.
    Omdat dit onderzoek Vision beschrijft, zullen wij het sensoraspect van dit onderzoek niet verwerken. In plaats daarvan gaan we dieper in op de referenties gemaakt in het paper met betrekking tot Vision.

    \paragraph{Aantal foto's per tijdseenheid}

    \paragraph{Hardware-vereisten}

    \emph{Bij de onderstaande redenering gaan wij uit van onze ervaring in algemene gezichtsdetectie, aangezien wij geen diepgaande ervaring hebben met gezichtseigenschappen-detectie.}

    \paragraph{Langer analyseren frame}

    \subsection{Deelvraag 5 }\label{subsec:deelvraag-5}
    \emph{Wat is de meest geschikte camera voor het toepassen van Computer Vision facial tracking algoritmes met embedded systemen die verkrijgbaar is op de markt?}

    Bij het vergelijken van de camera's gebruiken wij de volgende attributen.

    \vspace{1em}

    \noindent \underline{Resolutie: Minimale resolutie van 50px*50px} \hfill Must have\\
    Deze resolutie is minimaal om accurate resultaten te krijgen met Vision\footnote{Zoals beschreven in \citet{boom2006effect}}

    \noindent \underline{Stroomverbruik: Minder dan 1 W stroomverbruik} \hfill Must have\\
    Er is op een los bewegende robot geen grote stroombron aanwezig om een camera continu aan te sturen.

    \noindent \underline{Universeel: Universele aansluiting} \hfill Should have\\
    De camera moet op een standaarplatform kunnen draaien, zodat er geen converters/chips aan het systeem toegevoegd moeten worden

    \noindent \underline{Lichtgewicht: lichter dan 300g} \hfill Should have\\
    De camera moet gedragen kunnen worden door een R2D2 robot

    \noindent \underline{Formaat: kleiner dan 10x5x5cm} \hfill Should have\\
    De camera moet niet groter zijn dan het board wat hem aanstuurt, zo houden we de module klein

    \noindent \underline{Vision: Kan zelf gezichtstracking uitvoeren} \hfill Could have\\
    Als de camera zelf facial tracking technieken kan uitvoeren, kunnen wij de hardware-vereisten van de verbonden microcontroller beperken

    \noindent \underline{Monteren: Met montagebeugel of soortgelijk} \hfill Could have\\
    Als de camera geleverd wordt met een makkelijke manier om hem te bevestigen, kan deze ook makkelijk op een R2D2 robot bevestigd worden


    \vspace{1em}

    We vergelijken de volgende camera's

    \paragraph{Pixy2}
    \href{https://pixycam.com/pixy2/}
    De pixy2 is een camera die specifiek gericht is op het gebruiken van Vision algoritmes in microcontrollers.
    Deze camera heeft veel ondersteunde communicatiemethoden die gebruikt kunnen worden met microcontrollers en embedded systemen.
    De camera heeft zelf een chip die lijnen, kleuren en specifieke objecten kan herkennen.
    Hiernaast kan via USB ook een live camerabeeld uitgelezen worden.


    \emph{Attributen verzameld via [\citenum{pixy_datasheet}]}

    \paragraph{Flir Firefly DL}
    \href{https://www.flir.com/products/firefly-dl/}
    De firefly DL is een Specifieke Computer Vision camera, gericht op vision door middel van Deep Learning.
    Dit is een camera die aangesloten kan worden op een computer om een neural network te trainen voor een bepaald doel.
    Hierna kan het getrainde netwerk geladen worden op de camera, die vervolgens via een USB verbinding de resultaten en het beeld kan doorsturen.


    \emph{Attributen verzameld via [\citenum{firefly_info}]}

    \paragraph{Webcam: Logitech C270}
    De makkelijkst verkrijgbare camera is natuurlijk een webcam. Deze doet niks voor ons in de vorm van Vision, en levert alleen beelden.
    Wel heeft deze een hele toegankelijke interface (USB2) die op veel embedded systemen beschikbaar is.

    \emph{Attributen verzameld via [\citenum{c270_datasheet}]}



    \makebox[0.9\textwidth]{
        \scriptsize{
        \begin{tabular}{l|c|c|c|c|c|c|c}
            \textbf{Camera} & \textbf{Resolutie} & \textbf{Stroomverbruik} & \textbf{Universeel} & \textbf{Lichtgewicht} & \textbf{Formaat} & \textbf{Vision} & \textbf{Monteren}\\
            \hline
            Pixy2 & 1296x976 & 0.495W  & SPI/I2C/UART/USB & 10g & 4.2x3.8x15 cm & Alleen Object tracking & Schroefgaten\\
            Firefly & 1440x1080 & 2.2W &USB 3.1 & 20g & 2.7x2.7x1.45 cm & Divers.\footnotemark[1] & getapte gaten\\
            C270 & 1280 x 720 & onbekend & USB & 226g & 7 x 3.1 x 2.1 \footnotemark[2] cm & Geen & Klemsysteem\\
        \end{tabular}
    }
    }
    \footnotetext[1]{Op een computer wordt een neural network getraind, wat geladen kan worden op de camera}
    \footnotetext[2]{Afmetingen uit het datasheet leken niet te kloppen, dus wij hebben dit handmatig opgemeten}

    \subsection{Deelvraag 6}\label{subsec:deelvraag-6}
    \emph{Welke bestaande implementaties van een pijnmeting algoritme zijn er, en wat zijn de gebruiksrechten hiervan?}

    Tijdens het onderzoek hebben wij helaas geen bestaande, bruikbare implementaties kunnen vinden van een van de onderzochte algoritmes.

    \subsection{Deelvraag 7}\label{subsec:deelvraag-7}
    \emph{Wat is de maximaal bereikbare accuraatheid en snelheid bij gebruik van boven onderzochte methoden?}

    Tijdens het implementeren van de gekozen opties hebben we helaas voor beiden opties geen werkende implementatie kunnen realiseren.
    In dit onderdeel beschrijven wij per implementatie waarom dit was.

    \subsubsection{Landmark Detection}
    Binnen de methode op basis van landmark detection is het gelukt om aan de hand van DLIB een landmark detection wrapper te schrijven.
    Na begonnen te zijn met de implementatie bleek echter dat deze methode gevoelig is voor beweging van het hoofd ten opzichte van de camera.
    Om dit te corrigeren hebben wij referentieafstanden ingesteld. Hierbij maken we gebruik van punten op het gezicht die altijd dezelfde onderlinge afstand hebben.
    Dit hebben wij opgesplitst in een horizontaal en een verticaal aspect. Het horizontale aspect is berekend vanuit de middelpunten van beiden ogen.
    Het verticale aspect is berekend op basis van de afstand van het puntje van de neus, tot de neusbrug.
    Dit hielp bij het corrigeren van de afstand van het gezicht tot de camera, maar bleek niet toereikend te zijn voor het corrigeren van de hoek van het gezicht tot de camera.

    \vspace{1em}

    \includegraphics[width=0.6\linewidth]{Images/Probleem_landmark.png}


    \vspace{1em}
    In de bovenstaande afbeelding is te zien welke effect dit heeft op de meetwaarden.
    Gebied A is de tijd waarin de persoon de ogen sterk samenknijpt. Alhoewel dit goed geregistreerd word, is het verschil met gebied B niet groot.
    Gebied B zijn hierbij de meetwaarden die opgepikt worden door het subtiel bewegen van het hoofd. Hierbij wordt het hoofd niet meer
    dan +- 10 graden in elke richting bewogen. Omdat de persoon in gebied A de ogen sterk samenknijpt verwachten wij niet accuraat te kunnen meten
    wanneer de ogen lichter samengeknepen worden.


    \subsubsection{Optical flow}
    Tijdens het onderzoek naar de optical flow methode is er gekeken naar verschillende types algoritmes. 
    Er zijn namelijk een hoop verschillende implementaties te vinden online. 
    Dit zijn zowel compleet verschillende ideeën, als libaries die gebruik zouden moeten versoepelen.
    Er zijn veel papers door gelezen, zoals \emph{\citet{naghsh2006efficient}} of artikels van \emph{\citet{Readinglist}}. 
    Er is veel tijd besteed aan literatuur onderzoek in de hoop om beter te begrijpen wat mogelijk is en welke algoritmes het beste zouden werken.
    Dit was een lastig process vanwege de vele en complexe wiskundige formules. 
    Deze waren voor ons doel niet heel intressant en hebben we zo veel mogelijk om heen geprobeerd te lezen en te focusen op de verschillende ideeën die naarvoren kwamen.
    Twee voornaamste ideeën die naar voren kwamen zijn de volgende.
    \begin{itemize}
        \item \emph{Corner tracking.} Het idee achter corner tracking is dat hoge kwaliteit zo genoemde hoeken worden gevonden en deze gevolgd worden. 
        Deze punten hoeven geen werkelijke hoeken te zijn en zijn voornamelijk hoog contrast punten. 
        Het achterliggende idee is dat als je aan het begin alleen hoog kwaliteit punten volgd je geen computerkracht hoeft te verspillen 
        aan het volgen van pixels die geen meerwaarde hebben of zelfs een negatieve impact hebben op de resultaten.
        \item \emph{full frame tracking.} 
        Full frame tracking is wanneer de optical flow calculatie wordt uitgevoerd op elke laatste pixel van een frame. 
        Dit is uiteraard intensiever maar kan betere resultaten opleveren voor met relatief hoge framerate beelden. 
        Dit laatste is wel een zekere eis omdat de minder goed gedefineerde pixels zo extreem kunnen veranderen dat deze 
        niet verder te volgen zijn als er te grote tijdsverschillen tussen frames zit.    
    \end{itemize}
    Als eerste heb ik de Corner gebaseerde tracking geprobeerd te implementeren.
    Hiervoor heeft opencv een functie en demo voor, welke ik gebruikt heb om mee te testen.
    Een probleem waar ik snel tegen aan liep is dat het lastig was om zelf punten aan te wijzen die het algoritme moest volgen.
    Dit is logisch, gezien het hele idee is dat een preprocessing stap goede punten selecteerd. 
    Dit maakte het werk echter wel lastiger omdat precies de gezichtsfeatures die getrackt moesten worden, niet geselecteerd werden. 
    De functie die de punten selecteerd heeft een hoop parameters en hiermee is gespeeld in de hoop toch goede punten te vinden in intressante gebieden als de wenkbrouwen.
    De voornaamste binnen het gezicht gevonden punten waren op de rand met de ogen, gezien het grote contrast tussen huid, oogwit, de pupil en iris.
    Hier onder is te zien dat zelfs met deze sterke punten tracking nogsteeds erg lastig was omdat de oogleden bepaalde punten bedekte. 
    Ook na het volledig heropenen van de ogen waren deze punten verloren, zoals te zien in de laatste foto. 
    \newline
    \includegraphics[width=0.6\linewidth]{Images/path21.png}
    \includegraphics[width=0.6\linewidth]{Images/path22.png}
    \includegraphics[width=0.6\linewidth]{Images/path23.png}
    \newline
    \newline
    Nadat dit eerste algoritme niet goed bleek te werken is er gewerkt aan het full frame tracking algoritme. 
    Deze bleek lastiger te implementeren en dus is \emph{\citet{pyflow}} \todo{fix this} gebruikt om dit te testen. 
    Helaas is deze library slecht gedocumenteerd en om een onbekende reden duurde het ook onredelijk lang om individele frames te analyseren.
    Deze library is een wrapper en dus is naar de bron gegaan, namelijk een dlib functie die de optical flow visualisaties van hele videos kan maken.
    Echter bleek ook snel al dat hier problemen mee zouden komen. 
    Onderstaands zijn enkele frames uit de output van deze functie te zien, en hier is duidelijk te zien hoe soms de beweging van de wenkbrouwen wel zichtbaar is, en hoe deze soms verdwijnt.
    Dit maakte ook dit algoritme helaas ongeschikt voor het volgen van facial features.
    \includegraphics[width=0.6\linewidth]{Images/opticalflow1.png}
    \includegraphics[width=0.6\linewidth]{Images/opticalflow2.png}

    \newline
    De voornaamste reden van de ondervonden problemen lijkt te zijn dat het vervormen van het gezicht, wat is wat we willen registeren, te veel verschil oplevert in de textuur van de huid.
    Zo wordt optical flow veel gebruikt in stabalisatie algoritme om bewegingen van een scene te bepalen, echter is die scene meestal statisch. 
    Op het moment dat wij dan gezichtsfeatures willen volgen gaat dit om dynamische situaties.
    Vooral onder imprefecte belichting levert rimpeling van de huid nieuwe schaduwen en highlights op op de huid. 
    Dit is natuurlijk desastreus voor een algoritme dat in essentie pixelwaarde probeert te volgen over verloop van tijd, gezien deze waarde constant veranderen 


    \section{Conclusie en Discussie}\label{sec:conclusie-en-discussie}

    \subsection{Vergelijking algoritmen}
    Zoals aangegeven bij de resultaten, hebben wij beiden methoden helaas niet volledig kunnen implementeren.
    Op basis van de code kunnen wij daarom geen conclusie trekken over de meest geschikte methode.

    Wel is de indruk gewekt dat, alhoewel de methode met landmark detection in de huidige implementatie niet toereikend is,
    de optical flow methode los van de implementatie ook niet goed zou werken. Dit denken wij, omdat de problemen waar wij met optical
    flow tegenaan zijn gelopen gerelateerd zijn aan het lage contrast tussen gezichtsvlakken, en niet aan de implementatie zelf.


    \subsection{Vergelijking camera's}
    Voor de vergelijking van camera's zijn we het erover eens dat voor gebruik bij computer vision de Firefly-DL de meest geschikte camera is.
    Door de Deep Learning mogelijkheden kan een algoritme dat gezichtskenmerken detecteert veel aangepast worden om precieze resultaten te leveren.
    Deze camera is echter minder geschikt voor embedded systemen door het hogere stroomverbruik en minder bruikbare interface.
    Er zijn namelijk weinig embedded systemen met een USB 3.1 aansluiting\footnotemark[3]

    \footnotetext[3]{Mogelijk werkt de camera wel met USB 2, maar hier konden wij niks over vinden in de gegevens, en wij konden dit niet testen zonder de camera}

    De Pixy2 is op dit vlak geschikter, aangezien de interfacemogelijkheden specifiek gericht zijn op embedded systemen.
    De Pixy voegt echter binnen de scope weinig toe voor gezichtskenmerk-tracking, aangezien het alleen vooraf bekende objecten/kleuren kan herkennen.

    Om niet meer functionaliteit te kopen dan nodig is, is daarom onze keuze gevallen op de standaard webcam.
    Doordat de camera zelf geen Vision mogelijkheden heeft, zal er een sterker embedded systeem nodig zijn \hyperref[meth1-hardware]{(zie hardware-vereisten)} om de algoritmen uit te voeren.
    Een groot voordeel, zeker in coronatijd is dat de meeste mensen wel een webcam thuis hebben staan, en de resultaten van dit onderzoek daarom goed reproduceerbaar zijn.


    \section{Evaluatie}\label{sec:evaluatie2}







    \section{Aanbevelingen en Suggesties voor verder onderzoek}\label{sec:aanbevelingen2}
    Omdat uit dit onderzoek geen beschreven werkende implementatie is voortgekomen, en wij niet denken dat wij met dit onderzoek alle
    mogelijkheden voor pijndetectie op basis van Vision hebben uitgeput, raden wij een lezer
    aan om een vervolgonderzoek te organiseren als zij daarin geïnteresseerd zijn. 


    \paragraph{Andere methoden} Een eerste mogelijke vervolgonderzoek zou naar waarschijnlijkheid niet besproken methoden beschrijven of combinaties waar wij geen tijd en gelegenheid voor hebben gevonden.
    Alhoewel wij deze niet hebben kunnen vinden binnen onze research periode verwachten wij dat er meer methoden zijn om door middel van Computer Vision een pijndetectie-algoritme te implementeren. 
    Ook is het misschien mogelijk om een beter functionerende versie van optical tracking te vinden, gezien hier meer literatuur voor beschrikbaar is dan we de tijd hebben gehad om door te spitten.

    \paragraph{Landmark Detection} Verder zou de lezer een onderzoek kunnen starten wat
    gaat over het verbeteren van de op landmarkdetectie gebaseerde methode. Onze gekozen implementatie zou
    een stuk beter kunnen werken wanneer er rekening gehouden wordt met de pose waarin het gezicht zich bevindt.
    Een vervolgonderzoek zou dan kunnen gaan over het gebruik van 3D pose estimation, waarbij de houding van het hoofd
    in 3d ruimte geschat wordt, om zo metingen te kunnen corrigeren door middel van matrix translaties, om zo altijd te
    rekenen alsof het slachtoffer recht in de camera kijkt.
    Een andere suggestie is om optical flow te gebruiken om deze correcties te detecteren.
    Gezien aanwezige codebases en demos zou dit een logische volgende stap kunnen zijn.

    \paragraph{Optical Flow} Er is nog veel te lezen over optical flow methodes en hoe je deze effectief implementeert.
    Wij hebben hierover een \emph{\citet{Readinglist} Reading list} gevonden.
    Hier staan op dit moment 72 artikels in over optical flow technieken. 
    Dit is zeker geen complete lijst met onderzoeken maar is zeker wel intressant lees materiaal.


    \paragraph{Pixy2} Ondanks dat de pixy camera geen gezichtskenmerken kan herkennen, is het misschien wel mogelijk om een algoritme efficienter te laten werken door de pixy voorwerk te laten doen.
    Er kan onderzocht worden of het mogelijk is met de Pixy om oninteressante gebieden uit te sluiten van berekening in een algoritme.
    Hierdoor kunnen algoritmen mogelijk op minder sterke microcontrollers uitgevoerd worden, of sneller resultaten leveren.




    \section{Bijlagen}\label{sec:bijlagen}

    \subsection{Invullingen onderzoek}\label{subsec:invullingen-onderzoek}

    \subsubsection{Invulvelden}
    In het volgende onderdeel beschrijven wij per methode de antwoorden op deze invulvelden.
    Hierbij interpreteren wij resultaten, en trekken conclusies.
    Deze resultaten hoeven dus niet direct uit het onderzoek overgenomen te zijn.

    \subsubsection{Methode 1}
    Deze methode is gebaseerd op het onderzoek (\citet{werner2014automatic}).
    Deze methode beschrijft pijndetectie door middel van het interpreteren van de afstand tussen specifieke gezichtskenmerken.

    \paragraph{Korte Omschrijving}
    Het paper omschrijft een bredere methode, maar wij leggen de focus op het gedeelte vision.
    Deze methode detecteert pijn door te kijken naar de positie van bepaalde gezichtseigenschappen ten opzichte van elkaar.
    Zo kan pijn zich bijvoorbeeld uiten in een "samengetrokken" gezicht.

    \paragraph{Van welk principe maakt het gebruik?}
    Relationele positie van gezichtseigenschappen

    \paragraph{Geschatte piek-geheugengebruik per meting}
    Wij gaan uit van een ruime minimum voor gezichtsdetectie.
    Gezichtsdetectie en herkenning functioneren beduidend minder onder een resolutie van 50x50 pixels zoals beschreven in \citet{boom2006effect}.
    Hier gaan wij dan ook vanuit bij het geheugengebruik.
    Een afbeelding van 50x50 gebruikt +- 7.5kB in ruimte.

    Methoden van facedetection zijn ons niet bekend, hierom is dit moeilijk in te schatten.
    Ervanuitgaande dat er convolutie-operaties uitgevoerd worden op de afbeelding zal de afbeelding hoogst waarschijnlijk op momenten meerdere keren in het geheugen moeten bestaand.
    Hiernaast moet de draaiende programmacode, en eventuele image kernels toegevoegd worden.
    In piek zal dit programma minimaal 50 kB geheugen gebruiken, maar naar alle waarschijnlijkheid meer.

    \paragraph{Geschatte tijdsduur voor 1 conclusie}
    Wij denken dat er minimaal 2 metingen gedaan moeten worden.
    Dit omdat het gezicht vergeleken moet worden, en de samentrekking van het gezicht ten opzichte van de eerste meting bepaald moet worden.

    Op een adequate computer hoeft een meting niet veel langer te duren dan 0,1 seconde.
    Twee metingen zouden dan zo'n 0.2 seconden duren.

    \paragraph{Embedded capaciteit}
    Zoals wij al enigszins hadden verwacht is het niet redelijkerwijs mogelijk dit te draaien op een Arduino.
    Dit is voornamelijk omdat het piek-geheugengebruik te groot is hiervoor.
    Wel is dit prima mogelijk om te draaien op een Raspberry Pi of een mini-pc.

    \paragraph{Maximale geschatte effectieve afstand}
    Dit is voornamelijk afhankelijk van de tijd die wij besteden aan het herkennen.
    In combinatie met een camera met een hoge resolutie kunnen afstanden van vele meters gehaald worden (uitgaande van een minimum gezichtsgrootte van 50pxx50px.
    Hiervoor zou een face detection algoritme wel erg vaak/erg precies uitgevoerd worden, wat de verwerkingstijd flink vergroot.
    De maximale afstand is in dit geval dus afhankelijk van de maximale verwerkingstijd

    \paragraph{Kosten vereiste specifieke apparatuur}
    Voor deze methode is alleen een (mini)-pc en een camera vereist.
    Een (minimale) combinatie van een Raspberry Pi (\euro{}40) en een standaard webcam(\euro{}30) zou ongeveer \euro{}70,- kosten.

    \paragraph{Haalbaarheid}
    Wij denken dat deze methode voor onze scope haalbaar is, met een paar aanpassingen:
    \begin{itemize}
        \item Deze methode projecteert gezichtseigenschappen op een 3d-model van een gezicht.
        Hier hebben wij noch ervaring, noch tijd voor om uit te werken.
        Om dit te vermijden zouden wij de aanname doen dat iemand recht in de camera kijkt.
        Zo kunnen wij toch met zekerheid afstanden tussen gezichtseigenschappen meten.
        \item Zoals eerder aangegeven richten wij ons enkel op het vision-gedeelte van dit onderzoek, niet op de biomedische sensoren.
    \end{itemize}

    \paragraph{Mogelijke implementatie}
    Wij maken gebruik van DLib, een c++ library waar een methode voor facial feature detection in opgenomen is.
    Uit een eerste foto/meting extraheren wij een aantal afstanden tussen bepaalde gezichtskenmerken, en slaan deze op.
    Hierna doen wij steeds opnieuw metingen en vergelijken dit met de oorspronkelijke afstandsmetingen,
    hierbij kijkend naar de specifieke veranderingen zoals genoteerd in de onderzochte papers.

    \subsubsection{Methode 2}

    \emph{\citet{barronperformance} Performance Of Optical Flow Techniques.}\\ \\
    \emph{\citet{naghsh2006efficient} An Efficient Algorithm for Motion Detection Based Facial Expression Recognition using Optical Flow.}\\ \\
    \emph{\citet{zainudinfacialexpression} Facial expression change detection algorithm using optical flow technique.}\\ \\

    \paragraph{Korte Omschrijving}
    Methode 2 draait om de zogenoemde techniek optical flow.
    Dit is een veel gebruikte techniek om verandering tussen frame in beeld te brengen en te kwantificeren.

    \paragraph{Van welk principe maakt het gebruik?}
    Deze techniek kijkt naar verschillen tussen frames en probeert de richting van verandering vast te stellen.
    De stappen die uitgevoerd worden zijn alsvolgd:
    \begin{enumerate}
        \item Er wordt een frame gebufferd.
        \item Er wordt een tweede frame gebufferd.
        \item Voor elke pixel in de eerste frame wordt in de tweede frame gekeken of deze kleur en intensiteit ongeveer te vinden is in de buurt van de oorspronkelijke pixel in de eerste frame.
        \item Op deze manier breng je beweging van pixels tussen frames in beeld.
        \item Vaak zijn er ook passes met grotere bins van pixels. Bijvoorbeeld van twee bij twee.
    \end{enumerate}
    Uiteraard is de werkelijke implementatie van dit soort algoritme veel complexer dan de bovenstaande stappen, maar in essentie komen ze bijna allemaal hier op neer.
    \newline
    \includegraphics[height=0.2\textheight]{Images/Vector-field-examples.jpg}
    \newline
    Bovenstaand is een voorbeeld te zien van hoe een array van vectoren voor te stellen is. 
    \newline
    \includegraphics[height=0.2\textheight]{Images/An-example-of-optical-flow-detection-of-face-and-gesture-Detected-optical-flow-vectors.jpg}
    \newline
    \emph{\citet{Exact_Algebraic_Method_of_Optical_Flow_Detection}}
    \newline
    Bovenstaand is een visualisatie van hoe dit er uit zou zien in een frame van een persoon en zijn beweging.
    Er wordt over het gezicht gekeken naar verandering, welke uitgedrukt worden in vectoren.
    Deze worden dan gematcht tegen een database met de verschillende emoties.
    Door dat je verandering in mensen hun gezicht kwantificeren. 

    \paragraph{Geschatte piek-geheugengebruik per meting}
    in \emph{\citet{barronperformance}} wordt gesproken over een 94\% nauwkurigheid bij een analyse van 5 frames en een 83,3\% nauwkurigheid.
    Gezien de ondergrens dus 3 frames is voor enige nauwkurigheid zijn de geheugen eisen voor het bewaren van de frames niet groot. 
    Dit moet prima te doen zijn op een Pi, zelfs met relatief grote frames. Ook laat het de ruimte om met meer frames meer 

    \paragraph{Geschatte tijdsduur voor 1 conclusie}
    De techniek kan met drie frames al een nauwkurigheid van 83\% opleveren volgens \emph{\citet{naghsh2006efficient}}.
    Deze drie frames zouden echter wel de verschillende staten van de emotie die uitgedrukt worden moeten in beeld moeten brengen.
    De geschatte tijdsduur zal dan ook grote deels afhankelijk zijn van de tijd die het kost om deze frames te verzamen.
    De analysetijd is echter wel relatief lang, niet in de buurt van real time op een embedded systeem, echter is dit geen requirement en dus geen probleem.

    \paragraph{Embedded capaciteit}
    De berekening die uitgevoerd moeten worden zijn een heleboel relatief simpele operaties.
    Een embedded systeem zou hier zeker wel relatief lang over doen (elke seconden), maar het is zeker niet onmogelijk.
    De precieze capaciteiten moeten worden vast gesteld met testen maar we hebben vertrouwen dat het mogelijk is om deze module te draaien op een Raspberry Pi.

    \paragraph{Maximale geschatte effectieve afstand}
    Je hebt geen hoge resolutie nodig om de gezichtseigenschappen te kunnen traceren met optical flow.
    Om deze methode goed uit te kunnen voeren op een embedded systeem wil je bij voorkeur een volledig gezicht zonder achtergrond.
    Om dit voor elkaar te krijgen zou je moeten het gezicht moeten uitvergroten.
    Op lage kwaliteit cameras levert dit meer ruis op, dit levert wel problemen op in optical flow berekening vanwege de instabiliteit van de gevolgde pixel.
    De benodigde berekeningen kunnen tot ongeveer 200x200 pixels uitgevoerd worden en met een standaard wide angle camera zou dit tot enkele meters haalbaar moeten zijn.

    \paragraph{Kosten vereiste specifieke apparatuur}
    We hebben hiervoor een camera (20 euro) en Raspberry Pi minimaal model 3B+ (35 euro), maar liever een model 4B (40 euro). 
    De voorkeur van Pi type komt van de wens om een zo krachtig mogelijke processor te hebben. 

    \paragraph{Haalbaarheid}
    Wij zijn twijfelachtig over de werkelijke haalbaarheid van deze methode, gezien de complexiteit van implementatie.
    Het zou haalbaar moeten zijn als we een paar voorwaarden hebben:
    \begin{itemize}
        \item Goed belichte, hoog contrast en laag ruis input beeld.
        \item In het input beeld alleen het gezicht met gezichtseigenschappen al getagt.
    \end{itemize}
    Een ander tevoorzien probleem zou kunnen zijn het gevolg van de vertrekking van de huid tijdens beweging. 
    Bijvoorbeeld tijdens het vertrekken van je wenkbrouwen komen erboven meer rimpels omdat je huid op elkaar wordt gedrukt. 
    Dit zou als gevolgen hebben dat er schaduwen onstaan en deze zouden de optical flow berekeningen kunnen verwarren. 

    \paragraph{Mogelijke implementatie}
    Een mogelijke implementatie zou berusten op enige bestaande libaries zoals libd of opencv.
    Dit omdat de implementatie van deze methode complex is en we het werk dat wij moeten doen zo veel mogelijk willen inperken tot wat nog niet bestaat.
    Dubbel werk doen is nooit handig, en al helemaal niet als het werk dat je wel moet doen buiten bekende perken ligt en waarschijnlijk zeer complex is.

    \bibliography{Include/Main}

\end{document}


